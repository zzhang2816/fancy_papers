<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>AETHER: 面向几何感知的统一世界建模</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Helvetica Neue', Helvetica, Arial, sans-serif;
      background-color: #f9f9fa;
      color: #1c1c1e;
      line-height: 1.6;
      padding: 40px;
      max-width: 960px;
      margin: auto;
    }
    h1, h2, h3 {
      font-weight: 600;
    }
    h1 {
      font-size: 2.4em;
      margin-bottom: 0.4em;
    }
    h2 {
      font-size: 1.8em;
      margin-top: 2em;
      margin-bottom: 0.5em;
    }
    h3 {
      font-size: 1.4em;
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    .equation {
      background: #fff;
      padding: 1em;
      border-left: 4px solid #007aff;
      margin: 1em 0;
      font-size: 1.1em;
    }
    code {
      background-color: #f1f1f3;
      padding: 2px 5px;
      border-radius: 4px;
    }
    .fig {
      background: #eaeaec;
      text-align: center;
      padding: 1em;
      margin: 1em 0;
      font-size: 0.9em;
      color: #666;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1em 0;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 8px;
      text-align: center;
    }
    th {
      background-color: #f4f4f5;
    }
  </style>
</head>
<body>
  <h1>AETHER: 面向几何感知的统一世界建模</h1>
  <p><strong>作者：</strong>Shanghai AI Laboratory</p>
  <p><strong>原文链接：</strong><a href="https://arxiv.org/abs/2503.18945" target="_blank">arXiv:2503.18945</a></p>
  <p><strong>项目主页：</strong><a href="https://aether-world.github.io" target="_blank">https://aether-world.github.io</a></p>

  <h2>研究动机</h2>
  <p>尽管世界模型在理解和预测环境方面取得进展，几何建模与生成模型间的整合仍面临挑战。大多数视频生成模型关注图像质量，而非物理一致性。AETHER 旨在打破这一壁垒，统一实现：</p>
  <ul>
    <li><strong>4D 重建：</strong>感知物体在三维空间和时间上的动态变化；</li>
    <li><strong>基于动作的视频预测：</strong>考虑相机轨迹条件下的未来帧生成；</li>
    <li><strong>视觉规划：</strong>通过观察图像与目标图像，实现路径规划和生成。</li>
  </ul>
  <p>尤其重要的是，AETHER 从未见过真实数据却能零样本泛化（zero-shot），凸显几何建模的强泛化能力。</p>

  <div class="fig">图1：AETHER 框架总览（请至原论文查阅示意图）</div>

  <h2>数学建模与核心算法</h2>
  <p>AETHER 基于 CogVideoX 视频扩散模型进行后训练，输入包括：</p>
  <ul>
    <li>视频颜色潜变量 \( z^c_0 \)</li>
    <li>深度视频潜变量 \( z^d_0 \)</li>
    <li>动作潜变量 \( z^a_0 \)</li>
  </ul>
  <p>所有潜变量进行通道维拼接，扩散过程训练目标为：</p>
  <div class="equation">\[ L_\theta = \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I),\ t \sim U(1, T)} \| \epsilon - \epsilon_\theta(z_t, t, c) \|^2 \]</div>
  <p>其中 \( z_0 = z^c_0 \otimes z^d_0 \otimes z^a_0 \)，\( c = c^c \otimes c^a \)。</p>

  <h3>深度视频处理</h3>
  <p>深度值通过以下公式转换为标准化视差：</p>
  <div class="equation">\[ x^{disp} = \frac{1}{\sqrt{\text{clip}(x^d, d_{min}, d_{max})}} \]</div>
  <div class="equation">\[ \hat{x}^{disp} = \frac{x^{disp} \times 2 - 1}{\max(x^{disp})} \]</div>
  <div class="equation">\[ z^d = E(\hat{x}^{disp} \otimes 1^3) \]</div>
  <p>最终与颜色潜变量通道拼接输入扩散模型。</p>

  <h3>相机轨迹处理（RayMap 表示）</h3>
  <p>将相机位姿矩阵（内参 \( K \)，外参 \( E \)）转换为每像素的光线起点 \( r_o \) 和方向 \( r_d \)：</p>
  <div class="equation">\[ t' = \frac{t}{\max(x^{disp})} \cdot s_{ray},\quad t_{log} = \text{sign}(t') \cdot \log(1 + |t'|) \]</div>
  <p>最终形成六通道的 raymap：三个方向 + 三个起点，输入动作编码器。</p>

  <h2>实验方法与训练设置</h2>
  <ul>
    <li><strong>数据：</strong>完全合成视频数据（RGB-D + 自动相机注释）</li>
    <li><strong>模型初始化：</strong>CogVideoX-5B-I2V</li>
    <li><strong>任务：</strong>重建（输入完整视频）、预测（输入观测图像）、规划（输入起止图像）</li>
    <li><strong>训练方式：</strong>任务条件随机mask，多任务共训</li>
    <li><strong>损失函数：</strong>两阶段。第一阶段为扩散 MSE，第二阶段加入 MS-SSIM、深度一致性、点云一致性</li>
    <li><strong>计算资源：</strong>80张A100-80GB，batch size 总计320</li>
  </ul>

  <h2>实验结果与核心结论</h2>
  <h3>4D重建性能</h3>
  <p>在 Sintel、BONN、KITTI 数据集上进行深度预测和相机姿态估计，AETHER 达到或超越多个 SOTA：</p>
  <div class="fig">表格略，详见论文 Table 1 & 2</div>
  <ul>
    <li>在 KITTI 上达成最小 AbsRel（0.054）和最高 δ&lt;1.25（97.7%）</li>
    <li>姿态估计 RPE trans 在 TUM-Dynamics 上最低（0.012）</li>
  </ul>

  <h3>视频生成与规划</h3>
  <ul>
    <li><strong>图像转视频：</strong>不依赖语言提示也可生成动态视频，优于 CogVideoX</li>
    <li><strong>动作条件生成：</strong>通过 raymap 精准控制相机移动</li>
    <li><strong>视觉导航：</strong>在无动作条件下仍能通过观察图像和目标图像实现规划路径</li>
  </ul>

  <div class="fig">图表略，详见论文 Table 3~6</div>

  <h2>评论：犀利点评</h2>
  <p><strong>优点：</strong></p>
  <ul>
    <li>提出统一框架实现三任务协同（重建、预测、规划）</li>
    <li>首次使用 raymap 表达相机轨迹进行控制</li>
    <li>完全训练于合成数据，却具备极强零样本泛化能力</li>
  </ul>
  <p><strong>不足与改进建议：</strong></p>
  <ul>
    <li>raymap 表示虽然有效，但对扩散模型兼容性存疑，导致姿态估计仍有噪声</li>
    <li>模型在室内数据上表现逊于室外，说明训练集偏向</li>
    <li>相较原模型，语言引导能力减弱，不适合复杂场景描述</li>
  </ul>

  <h2>总结</h2>
  <p>AETHER 作为一个<strong>几何感知、多任务、合成训练</strong>的世界模型，展示了 diffusion transformer 与几何建模协同的巨大潜力。该框架适合作为后续世界模型研究的模板，未来值得探索：</p>
  <ul>
    <li>更高效或更结构化的动作表示</li>
    <li>与真实数据联合训练</li>
    <li>融合语言模态以提升泛化与控制力</li>
  </ul>
</body>
</html>
